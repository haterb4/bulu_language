use bulu::lexer::{Lexer, TokenType, Literal};

#[test]
fn test_unicode_identifiers() {
    // Test that identifiers can contain Unicode characters
    let source = "caf√© na√Øve r√©sum√© ÂèòÈáè –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è";
    let mut lexer = Lexer::new(source);
    let tokens = lexer.tokenize().unwrap();
    
    assert_eq!(tokens[0].token_type, TokenType::Identifier);
    assert_eq!(tokens[0].lexeme, "caf√©");
    assert_eq!(tokens[1].token_type, TokenType::Identifier);
    assert_eq!(tokens[1].lexeme, "na√Øve");
    assert_eq!(tokens[2].token_type, TokenType::Identifier);
    assert_eq!(tokens[2].lexeme, "r√©sum√©");
    assert_eq!(tokens[3].token_type, TokenType::Identifier);
    assert_eq!(tokens[3].lexeme, "ÂèòÈáè");
    assert_eq!(tokens[4].token_type, TokenType::Identifier);
    assert_eq!(tokens[4].lexeme, "–ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è");
}

#[test]
fn test_unicode_strings() {
    let source = r#""Hello, ‰∏ñÁïå!" "Caf√© ‚òï" "üöÄ Rocket""#;
    let mut lexer = Lexer::new(source);
    let tokens = lexer.tokenize().unwrap();
    
    assert_eq!(tokens[0].literal, Some(Literal::String("Hello, ‰∏ñÁïå!".to_string())));
    assert_eq!(tokens[1].literal, Some(Literal::String("Caf√© ‚òï".to_string())));
    assert_eq!(tokens[2].literal, Some(Literal::String("üöÄ Rocket".to_string())));
}

#[test]
fn test_large_numbers() {
    let source = "2147483647 -2147483648 1000000000";
    let mut lexer = Lexer::new(source);
    let tokens = lexer.tokenize().unwrap();
    
    assert_eq!(tokens[0].literal, Some(Literal::Integer(2147483647)));
    assert_eq!(tokens[1].token_type, TokenType::Minus);
    assert_eq!(tokens[2].literal, Some(Literal::Integer(2147483648))); // This is parsed as positive
    assert_eq!(tokens[3].literal, Some(Literal::Integer(1000000000)));
}

#[test]
fn test_very_long_identifier() {
    let very_long_name = "a".repeat(1000);
    let mut lexer = Lexer::new(&very_long_name);
    let tokens = lexer.tokenize().unwrap();
    
    assert_eq!(tokens[0].token_type, TokenType::Identifier);
    assert_eq!(tokens[0].lexeme, very_long_name);
}

#[test]
fn test_very_long_string() {
    let very_long_content = "x".repeat(10000);
    let source = format!("\"{}\"", very_long_content);
    let mut lexer = Lexer::new(&source);
    let tokens = lexer.tokenize().unwrap();
    
    assert_eq!(tokens[0].token_type, TokenType::StringLiteral);
    assert_eq!(tokens[0].literal, Some(Literal::String(very_long_content)));
}

#[test]
fn test_deeply_nested_comments() {
    let mut source = String::new();
    source.push_str("let x = 42\n");
    
    // Create deeply nested comments
    for _ in 0..100 {
        source.push_str("/*");
    }
    source.push_str(" deeply nested ");
    for _ in 0..100 {
        source.push_str("*/");
    }
    
    source.push_str("\nlet y = 24");
    
    let mut lexer = Lexer::new(&source);
    let tokens = lexer.tokenize().unwrap();
    
    let non_newline_tokens: Vec<_> = tokens.iter()
        .filter(|t| t.token_type != TokenType::Newline && t.token_type != TokenType::Eof)
        .collect();
    
    // Should have: let, x, =, 42, let, y, =, 24
    assert_eq!(non_newline_tokens.len(), 8);
}

#[test]
fn test_mixed_line_endings() {
    let source = "let x = 42\nlet y = 24\r\nlet z = 36";
    let mut lexer = Lexer::new(source);
    let tokens = lexer.tokenize().unwrap();
    
    let newline_tokens: Vec<_> = tokens.iter()
        .filter(|t| t.token_type == TokenType::Newline)
        .collect();
    
    // Should have 2 newlines (\n and \r\n, \r is treated as whitespace)
    assert_eq!(newline_tokens.len(), 2);
}

#[test]
fn test_adjacent_operators() {
    let source = "++--**//==!=<=>=<<>><-->===>";
    let mut lexer = Lexer::new(source);
    let tokens = lexer.tokenize().unwrap();
    
    let expected_types = vec![
        TokenType::Plus, TokenType::Plus,           // ++
        TokenType::Minus, TokenType::Minus,         // --
        TokenType::Power,                           // **
        TokenType::Slash, TokenType::Slash,         // // (but this becomes a comment!)
        TokenType::Equal,                           // ==
        TokenType::NotEqual,                        // !=
        TokenType::LessEqual,                       // <=
        TokenType::GreaterEqual,                    // >=
        TokenType::LeftShift,                       // <<
        TokenType::RightShift,                      // >>
        TokenType::LeftArrow,                       // <-
        TokenType::RightArrow,                      // ->
        TokenType::Equal,                           // ==
        TokenType::FatArrow,                        // =>
        TokenType::Eof,
    ];
    
    // Note: The // will be treated as a comment, so we need to adjust expectations
    let actual_types: Vec<_> = tokens.iter().map(|t| t.token_type).collect();
    
    // The // should start a comment that goes to end of line, so we expect fewer tokens
    assert!(actual_types.len() < expected_types.len());
    assert_eq!(actual_types[0], TokenType::Plus);
    assert_eq!(actual_types[1], TokenType::Plus);
    assert_eq!(actual_types[2], TokenType::Minus);
    assert_eq!(actual_types[3], TokenType::Minus);
    assert_eq!(actual_types[4], TokenType::Power);
}

#[test]
fn test_zero_prefixed_decimals() {
    let source = "00 01 007 0123";
    let mut lexer = Lexer::new(source);
    let tokens = lexer.tokenize().unwrap();
    
    // These should be parsed as decimal numbers, not octal
    assert_eq!(tokens[0].literal, Some(Literal::Integer(0)));
    assert_eq!(tokens[1].literal, Some(Literal::Integer(1)));
    assert_eq!(tokens[2].literal, Some(Literal::Integer(7)));
    assert_eq!(tokens[3].literal, Some(Literal::Integer(123)));
}

#[test]
fn test_float_edge_cases() {
    let source = ".5 5. 0.0 .0 0. 1.0e0 1.e5 .5e-3";
    let mut lexer = Lexer::new(source);
    let tokens = lexer.tokenize().unwrap();
    
    // Check that all are recognized as floats
    for i in 0..8 {
        if tokens[i].token_type != TokenType::Dot {  // .5 might be parsed as . and 5
            assert!(
                tokens[i].token_type == TokenType::FloatLiteral || 
                tokens[i].token_type == TokenType::IntegerLiteral,
                "Token {} should be a number, got {:?}", i, tokens[i].token_type
            );
        }
    }
}

#[test]
fn test_string_with_all_escapes() {
    let source = r#""\n\t\r\\\"\'\0""#;
    let mut lexer = Lexer::new(source);
    let tokens = lexer.tokenize().unwrap();
    
    assert_eq!(tokens[0].token_type, TokenType::StringLiteral);
    assert_eq!(tokens[0].literal, Some(Literal::String("\n\t\r\\\"'\0".to_string())));
}

#[test]
fn test_empty_string_and_char() {
    let source = r#""" ''"#;  // Empty string, but char literals can't be empty
    let mut lexer = Lexer::new(source);
    let result = lexer.tokenize();
    
    // Empty string should work
    if let Ok(tokens) = result {
        assert_eq!(tokens[0].token_type, TokenType::StringLiteral);
        assert_eq!(tokens[0].literal, Some(Literal::String("".to_string())));
        
        // But the empty char literal should cause an error or be parsed differently
        // This depends on implementation - let's just check we don't crash
    }
}

#[test]
fn test_maximum_nesting_depth() {
    // Test with reasonable nesting depth to avoid stack overflow
    let mut source = String::new();
    for _ in 0..50 {
        source.push_str("/*");
    }
    source.push_str("content");
    for _ in 0..50 {
        source.push_str("*/");
    }
    
    let mut lexer = Lexer::new(&source);
    let tokens = lexer.tokenize().unwrap();
    
    // Should just have EOF after the comment
    assert_eq!(tokens.len(), 1);
    assert_eq!(tokens[0].token_type, TokenType::Eof);
}